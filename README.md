# AST-Augmented-Code-Llama-for-TCL
Large language models for code generation typically process programs as linear token sequences, implicitly learning syntactic structure from data. This paradigm is insufficient for control-oriented scripting languages such as Tool Command Language (TCL), where datasets are limited and syntactic correctness is tightly coupled with hierarchical command structure. We propose an AST-augmented Code Llama architecture that injects abstract syntax tree information directly at the embedding level. A dedicated AST embedding layer is introduced alongside token embeddings and hierarchical positional encodings, enforcing structural observability prior to attention.It was demonstrated that while zero-shot injection yields a minor distributional gap, a lightweight alignment phase reduces the negative log-likelihood (NLL) loss by 94.37%. This approach acts as a structural constraint rather than a decoder-side control mechanism, significantly improving the model's awareness of hierarchical command nesting and substitution syntax.
