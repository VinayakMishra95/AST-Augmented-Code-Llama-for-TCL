{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. ENVIRONMENT STABILIZATION\n# Upgrading to ensure 2026-standard ABI 15 and 4-bit support\n!pip install -U -q bitsandbytes transformers accelerate tree-sitter\n!rm -rf tree-sitter-tcl\n!git clone https://github.com/tree-sitter-grammars/tree-sitter-tcl -q\n\nimport os, ctypes, torch, torch.nn as nn\nfrom tree_sitter import Language, Parser\nfrom transformers import LlamaForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom torch.amp import autocast, GradScaler\n\n# 2. COMPILE & BIND PARSER (ABI 15 COMPATIBLE)\n!mkdir -p build\n!gcc -shared -o build/tcl.so -fPIC -I./tree-sitter-tcl/src \\\n  ./tree-sitter-tcl/src/parser.c ./tree-sitter-tcl/src/scanner.c\n\nlib = ctypes.cdll.LoadLibrary('build/tcl.so')\ntcl_func = getattr(lib, \"tree_sitter_tcl\")\ntcl_func.restype = ctypes.c_void_p\nptr = tcl_func()\n\n# SMART INITIALIZATION: Detects 2024 (1 arg) vs 2026 (2 args) API\ntry:\n    TCL_LANGUAGE = Language(ptr, \"tcl\")\n    parser = Parser(TCL_LANGUAGE)\n    print(\"‚úÖ Parser: Using modern API (0.22+)\")\nexcept TypeError:\n    TCL_LANGUAGE = Language(ptr)\n    parser = Parser(TCL_LANGUAGE)\n    print(\"‚úÖ Parser: Using stable API (0.21.x)\")\n\n# 3. DEFINE ARCHITECTURE (Additive Structural Injection)\nclass ASTEmbeddingLayer(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.type_embed = nn.Embedding(256, hidden_size)\n        self.depth_embed = nn.Embedding(64, hidden_size)\n        self.sibling_embed = nn.Embedding(128, hidden_size)\n        self.proj = nn.Linear(hidden_size, hidden_size)\n        self.layer_norm = nn.LayerNorm(hidden_size)\n\n    def forward(self, t, d, s):\n        # Kept in float32 for training stability\n        return self.layer_norm(self.proj(\n            self.type_embed(t) + self.depth_embed(d) + self.sibling_embed(s)\n        ))\n\nclass ASTAugmentedLlama(nn.Module):\n    def __init__(self, model_id=\"codellama/CodeLlama-7b-hf\"):\n        super().__init__()\n        self.base_model = LlamaForCausalLM.from_pretrained(\n            model_id, \n            quantization_config=BitsAndBytesConfig(load_in_4bit=True), \n            device_map=\"auto\"\n        )\n        self.ast_injector = ASTEmbeddingLayer(self.base_model.config.hidden_size).to(\"cuda\")\n\n    def forward(self, input_ids, node_types, depths, siblings, labels=None):\n        embeds = self.base_model.get_input_embeddings()(input_ids)\n        ast_signal = self.ast_injector(node_types, depths, siblings)\n        # Injection: e_final = e_token + e_AST\n        # Cast signal to float16 to match Llama layers and avoid RuntimeError\n        return self.base_model(inputs_embeds=embeds + ast_signal.to(embeds.dtype), labels=labels)\n\n# 4. DATA ENGINE (Character-to-AST Mapping)\ndef get_metadata(code, tokenizer):\n    tree = parser.parse(bytes(code, \"utf8\"))\n    enc = tokenizer(code, truncation=True, max_length=512, return_offsets_mapping=True, padding=\"max_length\")\n    node_types, depths, siblings = [], [], []\n    for start, end in enc['offset_mapping']:\n        if start == end: \n            node_types.append(0); depths.append(0); siblings.append(0); continue\n        node = tree.root_node.descendant_for_byte_range(start, end)\n        node_types.append(hash(node.type) % 256)\n        d, p = 0, node\n        while p.parent: d += 1; p = p.parent\n        depths.append(min(d, 63))\n        siblings.append(min(node.child_index if hasattr(node, 'child_index') else 0, 127))\n    return {k: torch.tensor([v]).to(\"cuda\") for k, v in {\"input_ids\": enc.input_ids, \"node_types\": node_types, \"depths\": depths, \"siblings\": siblings}.items()}\n\n# 5. EXECUTION & ALIGNMENT TRAINING\nprint(\"üöÄ Loading Model...\")\nmodel_id = \"codellama/CodeLlama-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = ASTAugmentedLlama(model_id)\n\ntarget_script = 'proc factorial {n} { if {$n <= 1} { return 1 } { return [expr {$n * [factorial [expr {$n-1}]]}] } }'\ninputs = get_metadata(target_script, tokenizer)\n\n# Dynamic Baseline Calculation (Zeroing AST)\nmodel.eval()\nwith torch.no_grad():\n    zero_signal = {k: torch.zeros_like(v) if k != \"input_ids\" else v for k, v in inputs.items()}\n    baseline_loss = model(**zero_signal, labels=inputs[\"input_ids\"]).loss.item()\n\n# Alignment Loop\noptimizer = torch.optim.AdamW(model.ast_injector.parameters(), lr=1e-4)\nscaler = GradScaler('cuda')\nmodel.train()\nprint(f\"‚ú® Training Structural Alignment (Dynamic Baseline: {baseline_loss:.4f})\")\n\nfor step in range(101):\n    optimizer.zero_grad()\n    with autocast('cuda'):\n        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n        loss = outputs.loss\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    if step % 50 == 0: print(f\"Step {step:03} | Alignment Loss: {loss.item():.4f}\")\n\n# 6. FINAL RESULTS\nmodel.eval()\nwith torch.no_grad():\n    final_loss = model(**inputs, labels=inputs[\"input_ids\"]).loss.item()\n\nprint(\"\\n--- FINAL QUANTITATIVE RESULTS ---\")\nprint(f\"Baseline Loss (No AST): {baseline_loss:.4f}\")\nprint(f\"Augmented Loss (Aligned): {final_loss:.4f}\")\nimprovement = ((baseline_loss - final_loss) / baseline_loss) * 100\nprint(f\"üìà Net Improvement: {improvement:.2f}%\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-15T07:38:32.100050Z","iopub.execute_input":"2026-01-15T07:38:32.100660Z","iopub.status.idle":"2026-01-15T07:42:38.240331Z","shell.execute_reply.started":"2026-01-15T07:38:32.100616Z","shell.execute_reply":"2026-01-15T07:42:38.239656Z"}},"outputs":[{"name":"stderr","text":"2026-01-15 07:39:03.401617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768462743.849667      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768462743.970552      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768462745.026872      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768462745.026908      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768462745.026911      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768462745.026913      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Parser: Using stable API (0.21.x)\nüöÄ Loading Model...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/1402359989.py:27: DeprecationWarning: int argument support is deprecated\n  TCL_LANGUAGE = Language(ptr)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f49d0b7b3e249f4a1618b2ccd6eb64e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99fc7c4f09bf45d5b18c94a0afa22e9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0295977f5f42739f4f4f475b859364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a189ca12e943bfa25951ea558eb39d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/637 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cef8bcb69ec4792a16e565cb5f3e159"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2fb4b1f4c6443b78bfb14f25499552b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3317558a3cf84600a17a3388645bfe93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8bbe967dede48a69df0a11f3918f5ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ff7b51f8c754806a9adcb83eaa08539"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d01bd66038f40dab8263ae04b6a6fb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0696e3bfaea4a74a60161e10485333c"}},"metadata":{}},{"name":"stdout","text":"‚ú® Training Structural Alignment (Dynamic Baseline: 7.3645)\nStep 000 | Alignment Loss: 7.3968\nStep 050 | Alignment Loss: 0.7423\nStep 100 | Alignment Loss: 0.4798\n\n--- FINAL QUANTITATIVE RESULTS ---\nBaseline Loss (No AST): 7.3645\nAugmented Loss (Aligned): 0.4755\nüìà Net Improvement: 93.54%\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# 1. DEFINE VALIDATION SUITE\nvalidation_suite = {\n    \"Recursive Factorial\": \"proc fact {n} { if {$n <= 1} { return 1 } { return [expr {$n * [fact [expr {$n-1}]]}] } }\",\n    \"Conditional Logic\": \"proc check {val} { set status 0; if {$val > 10} { set status 1 } else { set status -1 }; return $status }\",\n    \"List Manipulation\": \"proc process_list {items} { set result {}; foreach item $items { lappend result [string toupper $item] }; return $result }\"\n}\n\nresults = []\n\n# 2. RUN EVALUATION\nmodel.eval()\nprint(f\"{'Scenario':<25} | {'Baseline':<10} | {'Augmented':<10} | {'Gain (%)':<10}\")\nprint(\"-\" * 65)\n\nfor name, code in validation_suite.items():\n    inputs = get_metadata(code, tokenizer) # Using your existing helper\n    \n    with torch.no_grad():\n        with autocast('cuda'):\n            # Calculate Baseline (Zeroed AST)\n            zero_signal = {k: torch.zeros_like(v) if k != \"input_ids\" else v for k, v in inputs.items()}\n            baseline = model(**zero_signal, labels=inputs[\"input_ids\"]).loss.item()\n            \n            # Calculate Augmented (Aligned)\n            augmented = model(**inputs, labels=inputs[\"input_ids\"]).loss.item()\n            \n    gain = ((baseline - augmented) / baseline) * 100\n    results.append({\"Scenario\": name, \"Baseline\": baseline, \"Augmented\": augmented, \"Gain\": gain})\n    print(f\"{name:<25} | {baseline:<10.4f} | {augmented:<10.4f} | {gain:>8.2f}%\")\n\navg_gain = sum(r['Gain'] for r in results) / len(results)\nprint(\"-\" * 65)\nprint(f\"AVERAGE STRUCTURAL GAIN: {avg_gain:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T07:44:50.272784Z","iopub.execute_input":"2026-01-15T07:44:50.274019Z","iopub.status.idle":"2026-01-15T07:44:54.868910Z","shell.execute_reply.started":"2026-01-15T07:44:50.273985Z","shell.execute_reply":"2026-01-15T07:44:54.868257Z"}},"outputs":[{"name":"stdout","text":"Scenario                  | Baseline   | Augmented  | Gain (%)  \n-----------------------------------------------------------------\nRecursive Factorial       | 0.9266     | 0.4537     |    51.04%\nConditional Logic         | 0.8793     | 0.5145     |    41.49%\nList Manipulation         | 0.8964     | 0.5979     |    33.30%\n-----------------------------------------------------------------\nAVERAGE STRUCTURAL GAIN: 41.94%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ABLATION EXPERIMENT\nablation_scenarios = {\n    \"Baseline (Token Only)\": {\"t\": False, \"d\": False, \"s\": False},\n    \"Token + Node Type\": {\"t\": True, \"d\": False, \"s\": False},\n    \"Token + Type + Depth\": {\"t\": True, \"d\": True, \"s\": False},\n    \"Full (Type + Depth + Sibling)\": {\"t\": True, \"d\": True, \"s\": True}\n}\n\nprint(f\"{'Configuration':<30} | {'NLL Loss':<10} | {'Reduction (%)':<10}\")\nprint(\"-\" * 60)\n\nfor label, config in ablation_scenarios.items():\n    # Helper to mask features by zeroing them out\n    mask_inputs = inputs.copy()\n    if not config[\"t\"]: mask_inputs[\"node_types\"] = torch.zeros_like(inputs[\"node_types\"])\n    if not config[\"d\"]: mask_inputs[\"depths\"] = torch.zeros_like(inputs[\"depths\"])\n    if not config[\"s\"]: mask_inputs[\"siblings\"] = torch.zeros_like(inputs[\"siblings\"])\n    \n    with torch.no_grad():\n        with autocast('cuda'):\n            loss = model(**mask_inputs, labels=inputs[\"input_ids\"]).loss.item()\n    \n    reduction = ((baseline_loss - loss) / baseline_loss) * 100\n    print(f\"{label:<30} | {loss:<10.4f} | {reduction:>8.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T07:47:07.197714Z","iopub.execute_input":"2026-01-15T07:47:07.198281Z","iopub.status.idle":"2026-01-15T07:47:10.266587Z","shell.execute_reply.started":"2026-01-15T07:47:07.198253Z","shell.execute_reply":"2026-01-15T07:47:10.265933Z"}},"outputs":[{"name":"stdout","text":"Configuration                  | NLL Loss   | Reduction (%)\n------------------------------------------------------------\nBaseline (Token Only)          | 0.8964     |    87.83%\nToken + Node Type              | 0.7800     |    89.41%\nToken + Type + Depth           | 0.5979     |    91.88%\nFull (Type + Depth + Sibling)  | 0.5979     |    91.88%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch.nn.functional as F\nimport numpy as np\n\n# 1. RUN INFERENCE\nmodel.eval()\nwith torch.no_grad():\n    with torch.amp.autocast('cuda'):\n        # Baseline: Structural side-car zeroed\n        zero_out = {k: torch.zeros_like(v) if k not in [\"input_ids\"] else v \n                    for k, v in inputs.items()}\n        base_logits = model(**zero_out).logits\n        \n        # Augmented: Aligned structural signal\n        aug_logits = model(**inputs).logits\n\n# 2. ALIGN FOR NEXT-TOKEN PREDICTION (The Shift)\n# Logits at [i] predict Token at [i+1]\nlabels = inputs[\"input_ids\"][:, 1:].contiguous()       # Targets\nlogits_aug = aug_logits[:, :-1, :].transpose(1, 2)    # Predictions\nlogits_base = base_logits[:, :-1, :].transpose(1, 2)\n\n# 3. CALCULATE LOSS DELTA\nbase_loss = F.cross_entropy(logits_base, labels, reduction='none')\naug_loss = F.cross_entropy(logits_aug, labels, reduction='none')\ndiff = (base_loss - aug_loss).cpu().numpy()[0]\n\n# 4. MANUAL MASKING (Filtering out </s> tokens)\n# We find where the target IDs are NOT the EOS token (usually ID 2)\ntarget_ids = labels[0].cpu().numpy()\neos_token_id = tokenizer.eos_token_id or 2 \nvalid_indices = np.where(target_ids != eos_token_id)[0]\n\n# Extract actual code tokens and their gains\nvalid_tokens = [tokenizer.convert_ids_to_tokens([target_ids[i]])[0] for i in valid_indices]\nvalid_diffs = diff[valid_indices]\n\n# 5. DISPLAY RESULTS (The Heatmap Data)\nprint(f\"{'Index':<6} | {'Token':<15} | {'Confidence Gain (Loss Delta)':<25}\")\nprint(\"-\" * 55)\nfor i in range(min(30, len(valid_tokens))):\n    t_clean = valid_tokens[i].replace(' ', ' ')\n    print(f\"{i:<6} | {t_clean:<15} | {valid_diffs[i]:>20.4f}\")\n\n# 6. AGGREGATED GAINS BY SYNTAX (Final Table 6 Data)\nsyntax_groups = {\n    \"Control Flow (proc, if)\": [],\n    \"Structural Grouping ({ }, [ ])\": [],\n    \"Logic & Operations (expr, set)\": []\n}\n\nfor t, d in zip(valid_tokens, valid_diffs):\n    t_str = t.replace(' ', '').strip()\n    if t_str in ['proc', 'if', 'else', 'return', 'foreach']:\n        syntax_groups[\"Control Flow (proc, if)\"].append(d)\n    elif t_str in ['{', '}', '[', ']']:\n        syntax_groups[\"Structural Grouping ({ }, [ ])\"].append(d)\n    elif any(op in t_str for op in ['expr', 'set', '<=', '==', '*', '+']):\n        syntax_groups[\"Logic & Operations (expr, set)\"].append(d)\n\nprint(\"\\n--- AGGREGATED GAINS FOR TABLE 6 ---\")\nfor cat, gains in syntax_groups.items():\n    if gains:\n        print(f\"{cat:<30}: Mean Delta = {sum(gains)/len(gains):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T07:56:27.450477Z","iopub.execute_input":"2026-01-15T07:56:27.450822Z","iopub.status.idle":"2026-01-15T07:56:29.152992Z","shell.execute_reply.started":"2026-01-15T07:56:27.450796Z","shell.execute_reply":"2026-01-15T07:56:29.152360Z"}},"outputs":[{"name":"stdout","text":"Index  | Token           | Confidence Gain (Loss Delta)\n-------------------------------------------------------\n0      | <s>             |               0.0000\n1      | ‚ñÅproc           |               0.0000\n2      | ‚ñÅprocess        |               4.2500\n3      | _               |               3.8281\n4      | list            |               3.6328\n5      | ‚ñÅ{              |               6.7617\n6      | items           |               4.7891\n7      | }               |               6.3242\n8      | ‚ñÅ{              |               7.0898\n9      | ‚ñÅset            |               4.7656\n10     | ‚ñÅresult         |               4.7500\n11     | ‚ñÅ{};            |               5.2266\n12     | ‚ñÅforeach        |               4.5547\n13     | ‚ñÅitem           |               4.6250\n14     | ‚ñÅ$              |               5.2930\n15     | items           |               4.5234\n16     | ‚ñÅ{              |               6.5977\n17     | ‚ñÅla             |               4.0703\n18     | pp              |               5.0156\n19     | end             |               4.8359\n20     | ‚ñÅresult         |               4.6406\n21     | ‚ñÅ[              |               5.3320\n22     | string          |               3.7812\n23     | ‚ñÅtou            |               3.6953\n24     | pper            |               4.3984\n25     | ‚ñÅ$              |               5.3672\n26     | item            |               4.6484\n27     | ]               |               5.0938\n28     | ‚ñÅ};             |               4.5547\n29     | ‚ñÅreturn         |               4.7266\n\n--- AGGREGATED GAINS FOR TABLE 6 ---\nStructural Grouping ({ }, [ ]): Mean Delta = 5.7109\nLogic & Operations (expr, set): Mean Delta = 4.7656\n","output_type":"stream"}],"execution_count":11}]}